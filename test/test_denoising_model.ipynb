{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.015449Z",
     "start_time": "2025-05-07T05:25:27.162208Z"
    }
   },
   "source": [
    "from imports.models import *\n",
    "from imports.utils import *\n",
    "import adabound as Adabound\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "POOL_PROCESS = 23\n",
    "FILE_GEN_INDEX = 2\n",
    "pool = Pool(processes=POOL_PROCESS)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "import itertools"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.022277Z",
     "start_time": "2025-05-07T05:25:29.019169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "time_resolution = 0.004\n",
    "time_data = np.arange(0, 60, time_resolution)\n",
    "\n",
    "MAGNETIC_FIELD = 403.553\n",
    "GYRO_MAGNETIC_RATIO = 1.07*1000       # Unit: Herts\n",
    "WL_VALUE = MAGNETIC_FIELD*GYRO_MAGNETIC_RATIO*2*np.pi\n",
    "N_PULSE_32 = 32\n",
    "N_PULSE_256 = 256\n",
    "\n",
    "SAVE_MLISTS = './'\n",
    "\n",
    "N_SAMPLES_TRAIN = 8192*4\n",
    "N_SAMPLES_VALID = 8192\n",
    "data_size = 1024\n",
    "GPU_INDEX = '0'"
   ],
   "id": "5a7dc5a9fee8375b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.039084Z",
     "start_time": "2025-05-07T05:25:29.035028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gaussian_slope(time_data, time_index, px_mean_value_at_time, M_list=None):\n",
    "    m_value_at_time = (px_mean_value_at_time * 2) - 1\n",
    "    Gaussian_co = -time_data[time_index] / np.log(m_value_at_time)\n",
    "\n",
    "    slope = np.exp(-(time_data / Gaussian_co)**2)\n",
    "    if M_list != None:\n",
    "        M_list_slope = M_list * slope\n",
    "        px_list_slope = (1 + M_list_slope) / 2\n",
    "        return px_list_slope, slope\n",
    "    return slope\n",
    "\n",
    "SLOPE_INDEX = 11812\n",
    "MEAN_PX_VALUE = np.linspace(0.65, 0.94, 20)\n",
    "slope = {}\n",
    "for idx, mean_px_value in enumerate(MEAN_PX_VALUE):\n",
    "    slope[idx] = gaussian_slope(time_data[:24000], SLOPE_INDEX, mean_px_value, None)"
   ],
   "id": "a3494edb87e934ab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.053733Z",
     "start_time": "2025-05-07T05:25:29.050860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Range of A & B (Hz)\n",
    "A_search_min = -80000\n",
    "A_search_max = 80000\n",
    "B_search_min = 2000\n",
    "B_search_max = 100000\n",
    "A_steps = 500\n",
    "B_steps = 500\n",
    "\n",
    "n_A_samples = (A_search_max - A_search_min) // A_steps\n",
    "n_B_samples = (B_search_max - B_search_min) // B_steps\n",
    "\n",
    "TOTAL_TEST_ARRAY = np.zeros((n_A_samples * n_B_samples, 2))"
   ],
   "id": "a8f345f8bdd3c9db",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.068780Z",
     "start_time": "2025-05-07T05:25:29.065464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "B_FIXED_VALUE = B_search_min\n",
    "for i in range(n_B_samples):\n",
    "    test_array = np.array([np.arange(A_search_min, A_search_max, A_steps), np.full((n_A_samples,), B_FIXED_VALUE)])\n",
    "    test_array = test_array.transpose()\n",
    "    test_array = np.round(test_array,2)\n",
    "    TOTAL_TEST_ARRAY[i*n_A_samples:(i+1)*n_A_samples] = test_array\n",
    "    B_FIXED_VALUE += B_steps"
   ],
   "id": "ff9e4650f8c8b526",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.083336Z",
     "start_time": "2025-05-07T05:25:29.079343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_candidate_max = 80000\n",
    "A_candidate_min = -80000\n",
    "B_candidate_max = 90000\n",
    "B_candidate_min = 2000\n",
    "\n",
    "A_small_max = 70000\n",
    "A_small_min = -70000\n",
    "B_small_max = 15000\n",
    "B_small_min = 2000\n",
    "\n",
    "candidate_boolen_index = ((TOTAL_TEST_ARRAY[:,1]>=B_candidate_min) & (TOTAL_TEST_ARRAY[:,1]<=B_candidate_max))   \\\n",
    "                        & ((TOTAL_TEST_ARRAY[:,0]>=A_candidate_min) & (TOTAL_TEST_ARRAY[:,0]<=A_candidate_max))\n",
    "AB_candidate_array = TOTAL_TEST_ARRAY[candidate_boolen_index]\n",
    "\n",
    "small_AB_boolen_index = ((TOTAL_TEST_ARRAY[:,1]>=B_small_min) & (TOTAL_TEST_ARRAY[:,1]<=B_small_max))   \\\n",
    "                         & ((TOTAL_TEST_ARRAY[:,0]>=A_small_min) & (TOTAL_TEST_ARRAY[:,0]<=A_small_max))\n",
    "small_AB_array = TOTAL_TEST_ARRAY[small_AB_boolen_index]\n",
    "\n",
    "AB_candidate_array *= (2 * np.pi)\n",
    "small_AB_array *= (2 * np.pi)"
   ],
   "id": "a5970d30ad4ed871",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.557977Z",
     "start_time": "2025-05-07T05:25:29.098388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_of_cases_train = 128\n",
    "n_of_samples = N_SAMPLES_TRAIN // n_of_cases_train\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(n_of_cases_train):\n",
    "\n",
    "    total_n_of_spin_lists = np.arange(24, 33)\n",
    "    n_of_spins = np.random.choice(total_n_of_spin_lists)\n",
    "    n_of_small_spins = n_of_spins // 2\n",
    "    n_of_spins, n_of_small_spins\n",
    "\n",
    "    globals()['ABlists_train_{}'.format(i)] = np.zeros((n_of_samples, n_of_spins, 2))\n",
    "\n",
    "    for idx in range(n_of_samples):\n",
    "\n",
    "        indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_spins-n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        globals()['ABlists_train_{}'.format(i)][idx, :n_of_spins-n_of_small_spins] = AB_candidate_array[indices_candi]\n",
    "\n",
    "        indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        globals()['ABlists_train_{}'.format(i)][idx, n_of_spins-n_of_small_spins:] = small_AB_array[indices_candi]\n",
    "\n",
    "print(\"ABlists for training dataset is generated: {} s\".format(round(time.time() - tic, 3)))"
   ],
   "id": "22d0366b56c3e88e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABlists for training dataset is generated: 0.456 s\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.736478Z",
     "start_time": "2025-05-07T05:25:29.623850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_of_cases_valid = 32\n",
    "n_of_samples = N_SAMPLES_VALID // n_of_cases_valid\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(n_of_cases_valid):\n",
    "\n",
    "    total_n_of_spin_lists = np.arange(24, 36)\n",
    "    n_of_spins = np.random.choice(total_n_of_spin_lists)\n",
    "    n_of_small_spins = n_of_spins // 2\n",
    "    n_of_spins, n_of_small_spins\n",
    "\n",
    "    globals()['ABlists_valid_{}'.format(i)] = np.zeros((n_of_samples, n_of_spins, 2))\n",
    "\n",
    "    for idx in range(n_of_samples):\n",
    "\n",
    "        indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_spins-n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        globals()['ABlists_valid_{}'.format(i)][idx, :n_of_spins-n_of_small_spins] = AB_candidate_array[indices_candi]\n",
    "\n",
    "        indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        globals()['ABlists_valid_{}'.format(i)][idx, n_of_spins-n_of_small_spins:] = small_AB_array[indices_candi]\n",
    "\n",
    "print(\"ABlists for validation dataset is generated: {} s\".format(round(time.time() - tic, 3)))"
   ],
   "id": "e01e27c394ca6aec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABlists for validation dataset is generated: 0.109 s\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:25:29.839562Z",
     "start_time": "2025-05-07T05:25:29.835125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def Px_noise_data(time_table, wL_value, AB_list, n_pulse, rand_idx, data_size, y_train_pure=False):\n",
    "    noise = 0.043                                    # Maximum Height of Noise\n",
    "    rescale = np.random.random()/2 + 0.75\n",
    "    noise *= rescale\n",
    "\n",
    "    AB_list = np.array(AB_list)\n",
    "    A = AB_list[:,0].reshape(len(AB_list), 1)\n",
    "    B = AB_list[:,1].reshape(len(AB_list), 1)\n",
    "\n",
    "    w_tilda = pow(pow(A+wL_value, 2) + B*B, 1/2)\n",
    "    mz = (A + wL_value) / w_tilda\n",
    "    mx = B / w_tilda\n",
    "\n",
    "    alpha = w_tilda * time_table.reshape(1, len(time_table))\n",
    "    beta = wL_value * time_table.reshape(1, len(time_table))\n",
    "\n",
    "    phi = np.arccos(np.cos(alpha) * np.cos(beta) - mz * np.sin(alpha) * np.sin(beta))\n",
    "    K1 = (1 - np.cos(alpha)) * (1 - np.cos(beta))\n",
    "    K2 = 1 + np.cos(phi)\n",
    "    K = pow(mx,2) * (K1 / K2)\n",
    "    M_list_temp = 1 - K * pow(np.sin(n_pulse * phi/2), 2)\n",
    "    Y_train = np.prod(M_list_temp, axis=0)\n",
    "\n",
    "    slope_temp = np.zeros(data_size)\n",
    "    if np.random.uniform() > 0.4:\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp[:data_size//3] = slope[temp_idx][rand_idx:rand_idx+data_size//3]\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp[data_size//3:2*(data_size//3)] = slope[temp_idx][rand_idx+data_size//3:rand_idx+2*(data_size//3)]\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp[2*(data_size//3):data_size] = slope[temp_idx][rand_idx+2*(data_size//3):rand_idx+data_size]\n",
    "\n",
    "    else:\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp = slope[temp_idx][rand_idx:rand_idx+data_size]\n",
    "\n",
    "    if y_train_pure == False:\n",
    "        Y_train = (1 + slope_temp*Y_train) / 2\n",
    "        X_train = Y_train + noise*(np.random.random(Y_train.shape[0]) - np.random.random(Y_train.shape[0]))\n",
    "        return X_train, Y_train\n",
    "\n",
    "    else:\n",
    "        Y_train_pure = (1 + copy.deepcopy(Y_train)) / 2\n",
    "        Y_train = (1 + slope_temp*Y_train) / 2\n",
    "        X_train = Y_train + noise*(np.random.random(Y_train.shape[0]) - np.random.random(Y_train.shape[0]))\n",
    "        return X_train, Y_train, Y_train_pure"
   ],
   "id": "731c878cf154839a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:09.584922Z",
     "start_time": "2025-05-07T05:25:29.846552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_of_cases = 128\n",
    "\n",
    "X_train = np.zeros((N_SAMPLES_TRAIN, data_size))\n",
    "Y_train = np.zeros((N_SAMPLES_TRAIN, data_size))\n",
    "Y_train_pure = np.zeros((N_SAMPLES_TRAIN, data_size))\n",
    "\n",
    "for i in range(n_of_cases):\n",
    "    print(i, end=' ')\n",
    "    for j in range(len(ABlists_train_0)):\n",
    "        rand_idx = np.random.randint(11000)\n",
    "        time_data_temp = time_data[rand_idx:rand_idx+data_size]\n",
    "\n",
    "        X_train[i*len(ABlists_train_0)+j], \\\n",
    "        Y_train[i*len(ABlists_train_0)+j], \\\n",
    "        Y_train_pure[i*len(ABlists_train_0)+j] \\\n",
    "        = Px_noise_data(time_data_temp, WL_VALUE, globals()['ABlists_train_{}'.format(i)][j], N_PULSE_32, rand_idx, data_size, y_train_pure=True)"
   ],
   "id": "6588cd470a6d24ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:20.769502Z",
     "start_time": "2025-05-07T05:26:09.635537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_valid = np.zeros((N_SAMPLES_VALID, data_size))\n",
    "Y_valid = np.zeros((N_SAMPLES_VALID, data_size))\n",
    "Y_valid_pure = np.zeros((N_SAMPLES_VALID, data_size))\n",
    "\n",
    "for i in range(int(N_SAMPLES_VALID/ABlists_valid_0.shape[0])):\n",
    "    for j in range(len(ABlists_valid_0)):\n",
    "        rand_idx = np.random.randint(11000)\n",
    "        time_data_temp = time_data[rand_idx:rand_idx+data_size]\n",
    "\n",
    "        X_valid[i*len(ABlists_valid_0)+j], \\\n",
    "        Y_valid[i*len(ABlists_valid_0)+j], \\\n",
    "        Y_valid_pure[i*len(ABlists_valid_0)+j], \\\n",
    "        = Px_noise_data(time_data_temp, WL_VALUE, globals()['ABlists_valid_{}'.format(i)][j], N_PULSE_32, rand_idx, data_size, y_train_pure=True)"
   ],
   "id": "46f6d98dbacc5843",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:20.827418Z",
     "start_time": "2025-05-07T05:26:20.822813Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, Y_train_pure.shape, Y_valid_pure.shape",
   "id": "6d9e58f879dea11d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32768, 1024),\n",
       " (32768, 1024),\n",
       " (8192, 1024),\n",
       " (8192, 1024),\n",
       " (32768, 1024),\n",
       " (8192, 1024))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:20.897636Z",
     "start_time": "2025-05-07T05:26:20.892514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import os, sys\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from imports.utils import *\n",
    "from imports.models import *\n",
    "from imports.adabound import AdaBound\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "import itertools"
   ],
   "id": "5275cc162f593f5b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:21.290480Z",
     "start_time": "2025-05-07T05:26:21.013762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = np.expand_dims(X_train, axis=-2)\n",
    "Y_train = np.expand_dims(Y_train, axis=-2)\n",
    "X_valid = np.expand_dims(X_valid, axis=-2)\n",
    "Y_valid = np.expand_dims(Y_valid, axis=-2)\n",
    "\n",
    "X_train = torch.Tensor(X_train.reshape(X_train.shape[0], 2, -1)).cuda()\n",
    "Y_train = torch.Tensor(Y_train.reshape(X_train.shape[0], 2, -1)).cuda()\n",
    "X_valid = torch.Tensor(X_valid.reshape(X_valid.shape[0], 2, -1)).cuda()\n",
    "Y_valid = torch.Tensor(Y_valid.reshape(Y_valid.shape[0], 2, -1)).cuda()"
   ],
   "id": "4acc88cd527b897a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:21.299282Z",
     "start_time": "2025-05-07T05:26:21.296032Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape",
   "id": "2ccce3a8b6914e71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32768, 2, 512]),\n",
       " torch.Size([32768, 2, 512]),\n",
       " torch.Size([8192, 2, 512]),\n",
       " torch.Size([8192, 2, 512]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:21.764991Z",
     "start_time": "2025-05-07T05:26:21.482430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Denoise_Model().cuda()\n",
    "try:\n",
    "    pred = model(X_train[:128])\n",
    "    print(pred.shape)\n",
    "except:\n",
    "    raise NameError(\"The input shape should be revised\")\n",
    "total_parameter = sum(p.numel() for p in model.parameters())\n",
    "print('total_parameter: ', total_parameter / 1000000, 'M')\n",
    "print(model)"
   ],
   "id": "ce1a46d79c2f6096",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 512])\n",
      "total_parameter:  0.03437 M\n",
      "Denoise_Model(\n",
      "  (conv1d_1): Conv1d(2, 64, kernel_size=(4,), stride=(1,), padding=(2,))\n",
      "  (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1d_2): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(2,))\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convTrans1d_3): ConvTranspose1d(64, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convTrans1d_4): ConvTranspose1d(64, 2, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:56.392223Z",
     "start_time": "2025-05-07T05:26:21.862834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'denoising_model_2.pt'\n",
    "epochs = 30\n",
    "train_batch = X_train.shape[0]\n",
    "mini_batch = 64\n",
    "valid_mini_batch = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss().cuda()\n",
    "\n",
    "total_loss = []\n",
    "total_val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "    tic = time.time()\n",
    "\n",
    "    for i in range(train_batch // mini_batch):\n",
    "        train_indices = np.random.choice(train_batch, size=mini_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X_train[train_indices])\n",
    "        cost = criterion(hypothesis, Y_train[train_indices])\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost\n",
    "        print(round(((i+1)*mini_batch)/train_batch*100), '% in Epoch', end='\\r')\n",
    "    loss_temp = avg_cost / (train_batch // mini_batch)\n",
    "    total_loss.append(loss_temp.cpu().detach().item())\n",
    "    print(\"Epoch:\", '%4d' % (epoch + 1), ' | Loss =', '{:.5f}'.format(loss_temp), end=' | ')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_indices = torch.randperm(X_valid.shape[0])[:valid_mini_batch]\n",
    "\n",
    "        prediction = model(X_valid[valid_indices])\n",
    "        val_loss = criterion(prediction, Y_valid[valid_indices])\n",
    "\n",
    "        print('Val_loss: {:.5f}'.format(val_loss.item()))\n",
    "        total_val_loss.append(val_loss.cpu().detach().item())\n",
    "    total_val_loss = np.array(total_val_loss)\n",
    "    if total_val_loss.min() >= total_val_loss[-1]:\n",
    "        torch.save(model.state_dict(), '../data/models/denoising_model.pt')\n",
    "    else:\n",
    "        if np.min(total_val_loss[-3:-1]) < total_val_loss[-1]:\n",
    "            learning_rate *= 0.5\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_val_loss = list(total_val_loss)"
   ],
   "id": "f222b192a4f4757b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1  | Loss = 0.04480 | Val_loss: 0.00784\n",
      "Epoch:    2  | Loss = 0.00617 | Val_loss: 0.00529\n",
      "Epoch:    3  | Loss = 0.00403 | Val_loss: 0.00323\n",
      "Epoch:    4  | Loss = 0.00291 | Val_loss: 0.00312\n",
      "Epoch:    5  | Loss = 0.00230 | Val_loss: 0.00232\n",
      "Epoch:    6  | Loss = 0.00194 | Val_loss: 0.00195\n",
      "Epoch:    7  | Loss = 0.00166 | Val_loss: 0.00186\n",
      "Epoch:    8  | Loss = 0.00147 | Val_loss: 0.00148\n",
      "Epoch:    9  | Loss = 0.00131 | Val_loss: 0.00135\n",
      "Epoch:   10  | Loss = 0.00121 | Val_loss: 0.00147\n",
      "Epoch:   11  | Loss = 0.00106 | Val_loss: 0.00117\n",
      "Epoch:   12  | Loss = 0.00095 | Val_loss: 0.00089\n",
      "Epoch:   13  | Loss = 0.00086 | Val_loss: 0.00101\n",
      "Epoch:   14  | Loss = 0.00080 | Val_loss: 0.00077\n",
      "Epoch:   15  | Loss = 0.00078 | Val_loss: 0.00080\n",
      "Epoch:   16  | Loss = 0.00074 | Val_loss: 0.00075\n",
      "Epoch:   17  | Loss = 0.00072 | Val_loss: 0.00079\n",
      "Epoch:   18  | Loss = 0.00071 | Val_loss: 0.00068\n",
      "Epoch:   19  | Loss = 0.00070 | Val_loss: 0.00068\n",
      "Epoch:   20  | Loss = 0.00069 | Val_loss: 0.00064\n",
      "Epoch:   21  | Loss = 0.00069 | Val_loss: 0.00064\n",
      "Epoch:   22  | Loss = 0.00069 | Val_loss: 0.00072\n",
      "Epoch:   23  | Loss = 0.00068 | Val_loss: 0.00066\n",
      "Epoch:   24  | Loss = 0.00068 | Val_loss: 0.00066\n",
      "Epoch:   25  | Loss = 0.00068 | Val_loss: 0.00065\n",
      "Epoch:   26  | Loss = 0.00068 | Val_loss: 0.00066\n",
      "Epoch:   27  | Loss = 0.00068 | Val_loss: 0.00065\n",
      "Epoch:   28  | Loss = 0.00068 | Val_loss: 0.00062\n",
      "Epoch:   29  | Loss = 0.00068 | Val_loss: 0.00067\n",
      "Epoch:   30  | Loss = 0.00068 | Val_loss: 0.00063\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:26:56.490675Z",
     "start_time": "2025-05-07T05:26:56.488944Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "11312ab4bd95bc28",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
